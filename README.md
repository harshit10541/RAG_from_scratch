## Retrieval-Augmented Generation (RAG) System
Engineered a RAG system to enhance LLM accuracy with private data, achieving 90%+ contextual relevance and reducing hallucinations by ~75%.

- Developed custom chunking and embedding generation (BAAI/bge-small-en-v1.5) for 100+ documents, creating a vector store of 250+ chunks.

- Implemented cosine similarity for efficient information retrieval, resulting in average query response times under 2 seconds.

- Integrated with Gemini 1.5 Flash (via LangChain) for robust, context-aware AI responses.

- Technologies: Python, Transformers, PyTorch, LangChain, Google Gemini API, NumPy.

